[
    {
        "id": 7479079,
        "user_id": 617558,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 961,
        "type": "question",
        "title": "Extra Credit Website, Special Participation D, Red Team",
        "content": "<document version=\"2.0\"><paragraph><link href=\"https://cs182-special-participation-d-red-team1.vercel.app/\">https://cs182-special-participation-d-red-team1.vercel.app/</link></paragraph><file url=\"https://static.us.edusercontent.com/files/CBHgFMU3hTUGNJJEqhc0BJRT\" filename=\"cs182-red-team-D.zip\"/><paragraph>Group Members: Alex Luu, Krish Yadav</paragraph><paragraph>We used Gemini 3.0 and Sonnet 4.5 to vibe code this site. In the first phase, we used an LLM to generate a Python script that interfaces with EdStem's API through the <code>edapi</code> package. It extracts titles, parses XML-formatted content, separates file attachments from external links, and automatically generates relevant tags based on keywords like homework numbers and technical terms. All extracted data is structured and exported to a JSON file. In the second phase, a new LLM session consumed the structured JSON data to build the complete web interface. This displays the posts in cards and also links back to the relevant EdStem posts. The site allows the user to filter by the generated tags.</paragraph><paragraph/></document>",
        "document": "https://cs182-special-participation-d-red-team1.vercel.app/\n\nGroup Members: Alex Luu, Krish Yadav\n\nWe used Gemini 3.0 and Sonnet 4.5 to vibe code this site. In the first phase, we used an LLM to generate a Python script that interfaces with EdStem's API through the edapi package. It extracts titles, parses XML-formatted content, separates file attachments from external links, and automatically generates relevant tags based on keywords like homework numbers and technical terms. All extracted data is structured and exported to a JSON file. In the second phase, a new LLM session consumed the structured JSON data to build the complete web interface. This displays the posts in cards and also links back to the relevant EdStem posts. The site allows the user to filter by the generated tags.\n\n",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 253,
        "unique_view_count": 75,
        "vote_count": 1,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-19T08:33:01.009359+11:00",
        "updated_at": "2025-12-20T08:28:02.441048+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": true,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": "2025-12-20T08:26:18.047308+11:00",
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 617558,
            "role": "user",
            "name": "Alex Luu",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7476064,
        "user_id": 957592,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 953,
        "type": "question",
        "title": "Extra Credit Special Participation D: Red Team 4",
        "content": "<document version=\"2.0\"><paragraph>Team Members: </paragraph><paragraph>Abdelaziz Mohamed</paragraph><paragraph>Zhaoxi Zhang<break/><break/><link href=\"https://aldurrah.github.io/muon-submissions-site/\">https://aldurrah.github.io/muon-submissions-site/</link></paragraph><paragraph/><paragraph/></document>",
        "document": "Team Members: \n\nAbdelaziz Mohamed\n\nZhaoxi Zhang\n\nhttps://aldurrah.github.io/muon-submissions-site/\n\n\n\n",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 376,
        "unique_view_count": 104,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-17T20:18:25.824491+11:00",
        "updated_at": "2025-12-20T08:28:24.765756+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": true,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": "2025-12-20T08:26:17.896708+11:00",
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 957592,
            "role": "user",
            "name": "Abdelaziz Mohamed",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7472857,
        "user_id": 647432,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 647432,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 943,
        "type": "question",
        "title": "Extra Credit Website: Blue Team Special Participation D",
        "content": "<document version=\"2.0\"><heading level=\"1\">Extra Credit Blue Team Special Participation D</heading><heading level=\"2\">Bruno Vieira and Daniel Kao</heading><paragraph><bold>Webiste:</bold> <link href=\"https://cs-182-ec.vercel.app/\">https://cs-182-ec.vercel.app/</link> </paragraph><paragraph><bold>Github repo link:</bold> <link href=\"https://github.com/brunobmv/CS182_EC\"><underline>https://github.com/brunobmv/CS182_EC</underline></link> </paragraph><paragraph/><heading level=\"1\">Introduction</heading><paragraph><bold>Motivation:</bold></paragraph><paragraph>We chose to build a searchable website of Special Participation D posts. To us, the meaningfulness of this category of posts is twofold: firstly, the reconfiguration of class materials for a Deep Learning landscape signifies the paradigm shifts in the field, such that the class is required to constantly update its materials to stay relevant. Secondly, the Special Participation D posts afforded students a unique opportunity to build course materials for the benefit of other people\u2019s learning. As such, students not only actively engaged with concepts currently on the Pareto front of Deep Learning (Muon, mup, SOAP, etc.), but they also have to think about how to redesign homework questions such that they are helpful to the learning of the average Deep Learning, thereby \u201cthinking like a teacher\u201d rather than as a passive learner.</paragraph><paragraph><bold>Features</bold></paragraph><paragraph>The website provides a searchable and filterable directory of student submissions and homework numbers, making it easy for the course staff to browse ideas by category or keyword. Submissions are displayed dynamically with clear descriptions, associated images, downloadable attachments, and links to student pages for visibility and commenting. The layout is lightweight, responsive, and easy to navigate, with all content loaded from a single data source. Since we have all the data stored in an Excel file, the site is fast to load, reliable, and simple to deploy on any standard web host.</paragraph><heading level=\"1\">Implementation</heading><paragraph>We built a dynamic website using HTML, CSS, and JavaScript for development, with a small Python script to handle data preparation. The script reads an Excel file containing all the data from the Ed posts, including students\u2019 names, date of post, homework number corresponding to the special participation, text from the post, Ed post code, post link, and finally file attachments. This is done using pandas to organize the submission information and use JSON to store all the data into a clean file that will be pulled on the website\u2019s simple frontend, where our JavaScript code loads the JSON dynamically. This allows us to display the submissions and support searching and filtering to aid the course staff to find student\u2019s submissions. All images and submission attachments (such as .pdf, .ipynb files) are stored in the website/data folder. The website runs as a simple single-page application with no external frameworks or backend, making it easy to host and maintain. We wrote code mainly using ChatGPT for suggestions, but most of the work was done by us, with very often times the help of Cursor when debugging a specific bug. However, we tried to make sure our ideas, design, and style were being manually added to the website by us (given we are in the Blue Team).</paragraph><heading level=\"1\">Reflection</heading><paragraph>The strengths of this website lie in its ease of use, stability, and cleanliness of its assets, and clear organization. By including both a dynamic search bar that serves posts based on student name and post title, and a filter for posts associated with specific homework, participation posts are easily accessed, and their connections to the timeline of the course are apparent. Students are also well credited in the posts themselves, as well as through the links that connect to each external Ed post; this also affords website viewers the opportunity to comment on the Ed posts and discuss the problem with other students there. We also created the Excel spreadsheet by hand, and thus made sure the data displayed is properly cleaned, and links/downloads/images were all functional. Pulling from this local Excel table also provides us the benefit that the website need not be maintained, assuming no additional Special Participation D posts are made. Each Ed post\u2019s information is fully contained in a single page and is organized cleanly and intuitively. </paragraph><paragraph> There do exist some areas of improvement for the website if we were given more time/resources. Firstly, the nature of the participation post means that students tend to upload large Python notebooks, and our implementation struggled to properly serve files larger than 10 MB. In this case, no files reached that size, and so the website is fully functional, but possibly hooking up the data in an external database like Firebase could be an option if given more time. Furthermore, the database being pulled from is a static Excel spreadsheet; to update the website, a user must manually update the spreadsheet; a significant source of friction. Thus, creating a backend portal for people to update the website, inputting information about their Ed post, could also be a useful feature to add for the website\u2019s continued relevance in the course. Finally, purchasing a domain name and subscribing to a hosting service would allow for the website to be far more accessible to an everyday person trying to learn more about Deep Learning.</paragraph></document>",
        "document": "Extra Credit Blue Team Special Participation D\n\nBruno Vieira and Daniel Kao\n\nWebiste: https://cs-182-ec.vercel.app/ \n\nGithub repo link: https://github.com/brunobmv/CS182_EC \n\n\n\nIntroduction\n\nMotivation:\n\nWe chose to build a searchable website of Special Participation D posts. To us, the meaningfulness of this category of posts is twofold: firstly, the reconfiguration of class materials for a Deep Learning landscape signifies the paradigm shifts in the field, such that the class is required to constantly update its materials to stay relevant. Secondly, the Special Participation D posts afforded students a unique opportunity to build course materials for the benefit of other people\u2019s learning. As such, students not only actively engaged with concepts currently on the Pareto front of Deep Learning (Muon, mup, SOAP, etc.), but they also have to think about how to redesign homework questions such that they are helpful to the learning of the average Deep Learning, thereby \u201cthinking like a teacher\u201d rather than as a passive learner.\n\nFeatures\n\nThe website provides a searchable and filterable directory of student submissions and homework numbers, making it easy for the course staff to browse ideas by category or keyword. Submissions are displayed dynamically with clear descriptions, associated images, downloadable attachments, and links to student pages for visibility and commenting. The layout is lightweight, responsive, and easy to navigate, with all content loaded from a single data source. Since we have all the data stored in an Excel file, the site is fast to load, reliable, and simple to deploy on any standard web host.\n\nImplementation\n\nWe built a dynamic website using HTML, CSS, and JavaScript for development, with a small Python script to handle data preparation. The script reads an Excel file containing all the data from the Ed posts, including students\u2019 names, date of post, homework number corresponding to the special participation, text from the post, Ed post code, post link, and finally file attachments. This is done using pandas to organize the submission information and use JSON to store all the data into a clean file that will be pulled on the website\u2019s simple frontend, where our JavaScript code loads the JSON dynamically. This allows us to display the submissions and support searching and filtering to aid the course staff to find student\u2019s submissions. All images and submission attachments (such as .pdf, .ipynb files) are stored in the website/data folder. The website runs as a simple single-page application with no external frameworks or backend, making it easy to host and maintain. We wrote code mainly using ChatGPT for suggestions, but most of the work was done by us, with very often times the help of Cursor when debugging a specific bug. However, we tried to make sure our ideas, design, and style were being manually added to the website by us (given we are in the Blue Team).\n\nReflection\n\nThe strengths of this website lie in its ease of use, stability, and cleanliness of its assets, and clear organization. By including both a dynamic search bar that serves posts based on student name and post title, and a filter for posts associated with specific homework, participation posts are easily accessed, and their connections to the timeline of the course are apparent. Students are also well credited in the posts themselves, as well as through the links that connect to each external Ed post; this also affords website viewers the opportunity to comment on the Ed posts and discuss the problem with other students there. We also created the Excel spreadsheet by hand, and thus made sure the data displayed is properly cleaned, and links/downloads/images were all functional. Pulling from this local Excel table also provides us the benefit that the website need not be maintained, assuming no additional Special Participation D posts are made. Each Ed post\u2019s information is fully contained in a single page and is organized cleanly and intuitively. \n\n There do exist some areas of improvement for the website if we were given more time/resources. Firstly, the nature of the participation post means that students tend to upload large Python notebooks, and our implementation struggled to properly serve files larger than 10 MB. In this case, no files reached that size, and so the website is fully functional, but possibly hooking up the data in an external database like Firebase could be an option if given more time. Furthermore, the database being pulled from is a static Excel spreadsheet; to update the website, a user must manually update the spreadsheet; a significant source of friction. Thus, creating a backend portal for people to update the website, inputting information about their Ed post, could also be a useful feature to add for the website\u2019s continued relevance in the course. Finally, purchasing a domain name and subscribing to a hosting service would allow for the website to be far more accessible to an everyday person trying to learn more about Deep Learning.",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 218,
        "unique_view_count": 81,
        "vote_count": 1,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-16T14:35:23.993003+11:00",
        "updated_at": "2025-12-20T08:29:01.719093+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 647432,
            "role": "user",
            "name": "Bruno Vieira",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7451971,
        "user_id": 957592,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 957592,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 890,
        "type": "question",
        "title": "Special Participation D: Shampoo + Adafactor for HW12",
        "content": "<document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/5DvumTUjh4YmQYLWzfcZ7DgY\" filename=\"hw12_participationD.pdf\"/><paragraph/></document>",
        "document": "",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 53,
        "unique_view_count": 23,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-11T17:37:21.05447+11:00",
        "updated_at": "2025-12-19T13:02:35.5207+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 957592,
            "role": "user",
            "name": "Abdelaziz Mohamed",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7451372,
        "user_id": 1288137,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 1288137,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 859,
        "type": "post",
        "title": "Special Participation D: Lion and SOAP in HW3",
        "content": "<document version=\"2.0\"><paragraph>I added subparts introducing and exploring the Lion and SOAP optimizers in HW3 coding. <break/><break/><bold>Summary:</bold><break/>Students are guided through implementing a simple version of each optimizer and comparing it to the other optimizers previously explored in the homework. In addition, code is given for small hyperparameter sweeps for both Lion and SOAP which students use to answer some written questions.<break/><break/></paragraph><file url=\"https://static.us.edusercontent.com/files/ros0kEFlnrwgXhArWY93hm1p\" filename=\"q_mup_coding.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/n5PCMOOkhGh98fAZGW8HVU85\" filename=\"q_mup_coding_solutions.ipynb\"/><paragraph/></document>",
        "document": "I added subparts introducing and exploring the Lion and SOAP optimizers in HW3 coding. \n\nSummary:\nStudents are guided through implementing a simple version of each optimizer and comparing it to the other optimizers previously explored in the homework. In addition, code is given for small hyperparameter sweeps for both Lion and SOAP which students use to answer some written questions.\n\n\n\n",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 36,
        "unique_view_count": 18,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-11T15:29:52.812491+11:00",
        "updated_at": "2025-12-19T10:35:25.704668+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 1288137,
            "role": "user",
            "name": "Justin Yang",
            "avatar": "bdj0jhK2yhSIC-P0Hax-t5JuivCRM8E2Rk2wD7NSSTM",
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7430922,
        "user_id": 959644,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 959644,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 709,
        "type": "post",
        "title": "Special Participation D: Experiment and Hyperparameter Sweep on SOAP and Lion Optimizers in HW 6",
        "content": "<document version=\"2.0\"><paragraph/><paragraph>In the attached notebook, I implemented two modern optimizers SOAP and Lion with coding agents and then integrated them with other optimizers used in the training tasks. I made a comparison between the performance of hand-picked learning rates with SOAP and Lion along with the performance of other optimizers presented in the notebook. Specifically, SOAP with a 1e-2 learning rate performs only better than Muon with 1e-2, and Lion has the fourth performance with a learning rate of 1e-3. Below is a graph with all of the optimizers used.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/AtE69FgiqciHCZgXccqCCZf2\" width=\"495\" height=\"245\"/></figure><paragraph>In addition, I also did a brief hyperparameter sweeping similar to the sweeps to Muon and AdamW. SOAP ends up having a 50.05% validation accuracy with a learning rate of 0.01, while Lion has the best validation rate of 68.61% at learning rate of 0.0005. It seems that overall, Lion has the best performance under this simple experiment.</paragraph><paragraph>--- Hyperparameter Sweep Results ---</paragraph><paragraph>SOAP:</paragraph><paragraph> (lr=0.01): 50.05%</paragraph><paragraph> (lr=0.005): 45.14%</paragraph><paragraph> (lr=0.001): 30.95%</paragraph><paragraph> (lr=0.0005): 25.58%</paragraph><paragraph>Best hyperparameters for SOAP: lr=0.01 with validation accuracy 50.05%</paragraph><paragraph>Lion:</paragraph><paragraph> (lr=0.01): 10.60%</paragraph><paragraph> (lr=0.005): 23.50%</paragraph><paragraph> (lr=0.001): 68.60%</paragraph><paragraph> (lr=0.0005): 68.61%</paragraph><paragraph>Best hyperparameters for Lion: lr=0.0005 with validation accuracy 68.61%</paragraph><paragraph>Google Colab Notebook:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1znNlvhV8TdtGcmJ-2xQt3N3syeuLcW_C/view?usp=sharing\"><underline>https://drive.google.com/file/d/1znNlvhV8TdtGcmJ-2xQt3N3syeuLcW_C/view?usp=sharing</underline></link></paragraph></document>",
        "document": "\n\nIn the attached notebook, I implemented two modern optimizers SOAP and Lion with coding agents and then integrated them with other optimizers used in the training tasks. I made a comparison between the performance of hand-picked learning rates with SOAP and Lion along with the performance of other optimizers presented in the notebook. Specifically, SOAP with a 1e-2 learning rate performs only better than Muon with 1e-2, and Lion has the fourth performance with a learning rate of 1e-3. Below is a graph with all of the optimizers used.\n\nIn addition, I also did a brief hyperparameter sweeping similar to the sweeps to Muon and AdamW. SOAP ends up having a 50.05% validation accuracy with a learning rate of 0.01, while Lion has the best validation rate of 68.61% at learning rate of 0.0005. It seems that overall, Lion has the best performance under this simple experiment.\n\n--- Hyperparameter Sweep Results ---\n\nSOAP:\n\n (lr=0.01): 50.05%\n\n (lr=0.005): 45.14%\n\n (lr=0.001): 30.95%\n\n (lr=0.0005): 25.58%\n\nBest hyperparameters for SOAP: lr=0.01 with validation accuracy 50.05%\n\nLion:\n\n (lr=0.01): 10.60%\n\n (lr=0.005): 23.50%\n\n (lr=0.001): 68.60%\n\n (lr=0.0005): 68.61%\n\nBest hyperparameters for Lion: lr=0.0005 with validation accuracy 68.61%\n\nGoogle Colab Notebook:\n\nhttps://drive.google.com/file/d/1znNlvhV8TdtGcmJ-2xQt3N3syeuLcW_C/view?usp=sharing",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 50,
        "unique_view_count": 22,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-08T17:37:19.801618+11:00",
        "updated_at": "2025-12-19T19:10:40.712434+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 959644,
            "role": "user",
            "name": "Jiayi Zhang",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7430849,
        "user_id": 959644,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 708,
        "type": "post",
        "title": "Special Participation D: Exploring Different Learning Rates and Batch Sizes on Homework 5",
        "content": "<document version=\"2.0\"><paragraph>In the attached notebook, I adjusted the CIFAR-10 task to make it adapt it to a parameter sweep on different learning rate and different batch sizes. The model is experiencing underfitting with the learning rates I have explored, and the batch sizes have a less effect on the performance under the underfitting than learning rates.</paragraph><paragraph>I also explore rerunning the experiments with both clean and cheating features on Muon's variant optimizers MuonLion and MuonAdamW. Since these optimizers are not available in Pytorch, I used ChatGPT 5.1 to help me to implement these two Muon variants. Both optimizers have a worse performance than traditional optimizers like SGD and Adam, and showed some interesting behaviors. I added some questions regarding to these optimizers to this task at the end.</paragraph><paragraph>Google Colab Notebook:</paragraph><paragraph><link href=\"https://drive.google.com/file/d/1JhwudZF6UMnLjLDnCRqoS0qZySEdzL2D/view?usp=sharing\">https://drive.google.com/file/d/1JhwudZF6UMnLjLDnCRqoS0qZySEdzL2D/view?usp=sharing</link></paragraph></document>",
        "document": "In the attached notebook, I adjusted the CIFAR-10 task to make it adapt it to a parameter sweep on different learning rate and different batch sizes. The model is experiencing underfitting with the learning rates I have explored, and the batch sizes have a less effect on the performance under the underfitting than learning rates.\n\nI also explore rerunning the experiments with both clean and cheating features on Muon's variant optimizers MuonLion and MuonAdamW. Since these optimizers are not available in Pytorch, I used ChatGPT 5.1 to help me to implement these two Muon variants. Both optimizers have a worse performance than traditional optimizers like SGD and Adam, and showed some interesting behaviors. I added some questions regarding to these optimizers to this task at the end.\n\nGoogle Colab Notebook:\n\nhttps://drive.google.com/file/d/1JhwudZF6UMnLjLDnCRqoS0qZySEdzL2D/view?usp=sharing",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 49,
        "unique_view_count": 24,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-08T17:21:12.239915+11:00",
        "updated_at": "2025-12-19T18:58:14.362698+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 959644,
            "role": "user",
            "name": "Jiayi Zhang",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7429801,
        "user_id": 607289,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 607289,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 690,
        "type": "post",
        "title": "Special Participation D: Lion and Muon in Homework 11",
        "content": "<document version=\"2.0\"><paragraph>HW 11 coding problem 4 (Scaling Laws of Batch Size) was focused on the effect of batch size on  optimal learning rates across SGD and Adam. I added Lion and Muon to the list of optimizers to investigate. While the deliverables are the same as for SGD and Adam, each of them presents a specific difficulty. </paragraph><paragraph>Lion isn't implemented in pytorch.optim, so students have to import it separately. Muon only takes 2D parameters, so students have to work around this. The new problems and my notebook with solutions are below (made use of Gemini in Colab). </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/2a7GYksrOAs07KoepeYkFMtf\" width=\"642\" height=\"95.06008583690988\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/R973dK6TPqDNFpYk3mVVggZ0\" width=\"642\" height=\"94.60085836909872\"/></figure><file url=\"https://static.us.edusercontent.com/files/f7lsJKNTSlY2gaUQXJmRPLOz\" filename=\"scaling_laws.ipynb\"/></document>",
        "document": "HW 11 coding problem 4 (Scaling Laws of Batch Size) was focused on the effect of batch size on  optimal learning rates across SGD and Adam. I added Lion and Muon to the list of optimizers to investigate. While the deliverables are the same as for SGD and Adam, each of them presents a specific difficulty. \n\nLion isn't implemented in pytorch.optim, so students have to import it separately. Muon only takes 2D parameters, so students have to work around this. The new problems and my notebook with solutions are below (made use of Gemini in Colab). ",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 36,
        "unique_view_count": 21,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-08T14:22:48.8061+11:00",
        "updated_at": "2025-12-19T17:45:28.713914+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 607289,
            "role": "user",
            "name": "Diana Kohr",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7428945,
        "user_id": 833750,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 833750,
        "accepted_id": 17293647,
        "duplicate_id": null,
        "number": 670,
        "type": "question",
        "title": "Special Participation D: Learning Rate with Batch Size Scaling on HW 12",
        "content": "<document version=\"2.0\"><paragraph>I extended Homework 12 by adding <bold>learning-rate scaling based on batch size</bold>, following the ideas from a recent 2024 paper on how learning rate should change when the batch size changes. This fits the goal of Special Participation Part D, which asks us to bring modern training practices into the assignment in a way that is easy to use and understand.</paragraph><paragraph>To do this, I updated both the q_vae.ipynb notebook and the code in the cs182hw12 folder. In utils.py, I implemented a small helper function called scale_learning_rate_with_batch_size that adjusts the learning rate depending on the batch size the model is using. It supports the two common rules people use in practice:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Linear scaling:</bold></paragraph><paragraph>$\\text{lr}{\\text{new}} = \\text{lr}\\text{base} \\cdot \\frac{B_{\\text{new}}}{B_{\\text{base}}}$</paragraph></list-item><list-item><paragraph><bold>Square-root scaling:</bold></paragraph><paragraph>$\\text{lr}{\\text{new}} = \\text{lr}\\text{base} \\cdot \\sqrt{\\frac{B_{\\text{new}}}{B_{\\text{base}}}}$</paragraph></list-item></list><paragraph/><paragraph>Then, in experiment.py, I applied this function so the optimizer automatically updates its learning rate based on the batch size chosen in the config. This lets the training loop behave more sensibly when using bigger or smaller batches, without needing to manually retune anything.</paragraph><paragraph>Overall, the changes are small and easy to toggle, but they make the VAE training setup feel much more \u201cmodern,\u201d since batch-size\u2013dependent learning-rate scaling is now a standard part of deep-learning practice.</paragraph><paragraph>Here are the zip files of the extended homework and solutions:</paragraph><file url=\"https://static.us.edusercontent.com/files/DxxpOoVipboaXJbff8WSOqFO\" filename=\"hw12_lr_batchsize.zip\"/><file url=\"https://static.us.edusercontent.com/files/h4WyJdan0egFCtiMRrNbStSW\" filename=\"hw12_lr_batchsize_sols.zip\"/></document>",
        "document": "I extended Homework 12 by adding learning-rate scaling based on batch size, following the ideas from a recent 2024 paper on how learning rate should change when the batch size changes. This fits the goal of Special Participation Part D, which asks us to bring modern training practices into the assignment in a way that is easy to use and understand.\n\nTo do this, I updated both the q_vae.ipynb notebook and the code in the cs182hw12 folder. In utils.py, I implemented a small helper function called scale_learning_rate_with_batch_size that adjusts the learning rate depending on the batch size the model is using. It supports the two common rules people use in practice:\n\nLinear scaling:\n\n$\\text{lr}{\\text{new}} = \\text{lr}\\text{base} \\cdot \\frac{B_{\\text{new}}}{B_{\\text{base}}}$\n\nSquare-root scaling:\n\n$\\text{lr}{\\text{new}} = \\text{lr}\\text{base} \\cdot \\sqrt{\\frac{B_{\\text{new}}}{B_{\\text{base}}}}$\n\n\n\nThen, in experiment.py, I applied this function so the optimizer automatically updates its learning rate based on the batch size chosen in the config. This lets the training loop behave more sensibly when using bigger or smaller batches, without needing to manually retune anything.\n\nOverall, the changes are small and easy to toggle, but they make the VAE training setup feel much more \u201cmodern,\u201d since batch-size\u2013dependent learning-rate scaling is now a standard part of deep-learning practice.\n\nHere are the zip files of the extended homework and solutions:",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 65,
        "unique_view_count": 28,
        "vote_count": 0,
        "reply_count": 1,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": true,
        "is_student_answered": false,
        "is_staff_answered": true,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-08T12:27:50.296354+11:00",
        "updated_at": "2025-12-19T17:03:06.785455+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 833750,
            "role": "user",
            "name": "Nazar Ospanov",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7428884,
        "user_id": 833750,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 833750,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 669,
        "type": "question",
        "title": "Special Participation D: Lion Optimizer on HW 12",
        "content": "<document version=\"2.0\"><paragraph>I extended Homework 12 by adding support for the <bold>Lion optimizer</bold>, a more modern and lightweight alternative to Adam. Since this option asks us to introduce new, up-to-date optimization methods into the assignment, I chose Lion because it reflects current trends in training, focusing on momentum and sign-based updates rather than heavier second-moment tracking.</paragraph><paragraph>To implement this, I extended the q_vae.ipynb notebook as well as modified the direct cs182hw12 folder. I created a new Lion optimizer class in <code>utils.py</code>. The class handles momentum updates, applies the sign() operation to determine the update direction, and includes weight decay. I followed the standard defaults suggested by the paper: <code>lr = 1e-4</code>, <code>betas = (0.9, 0.99)</code>, and <code>weight_decay = 1.0</code>. I then integrated Lion into the training pipeline by modifying build_optimizers in experiment.py so that setting <code>config.use_lion = 1</code> switches the model to Lion automatically, while all existing behavior stays the same when Lion is not enabled. This works for both the VAE and GAN configurations used in the homework.</paragraph><paragraph>Overall, this addition makes it easy to compare Adam and Lion in the same codebase and experiment with how a more modern optimizer performs, all while keeping changes minimal and easy to toggle.</paragraph><paragraph>Here are the zip files of the extended homework and solutions:</paragraph><file url=\"https://static.us.edusercontent.com/files/TZX8BfYOSpj7f18DBxcgMZ06\" filename=\"hw12_lion.zip\"/><file url=\"https://static.us.edusercontent.com/files/qmD5g9VAISDY4mDRyYv3DvwF\" filename=\"hw_12_lion_sols.zip\"/></document>",
        "document": "I extended Homework 12 by adding support for the Lion optimizer, a more modern and lightweight alternative to Adam. Since this option asks us to introduce new, up-to-date optimization methods into the assignment, I chose Lion because it reflects current trends in training, focusing on momentum and sign-based updates rather than heavier second-moment tracking.\n\nTo implement this, I extended the q_vae.ipynb notebook as well as modified the direct cs182hw12 folder. I created a new Lion optimizer class in utils.py. The class handles momentum updates, applies the sign() operation to determine the update direction, and includes weight decay. I followed the standard defaults suggested by the paper: lr = 1e-4, betas = (0.9, 0.99), and weight_decay = 1.0. I then integrated Lion into the training pipeline by modifying build_optimizers in experiment.py so that setting config.use_lion = 1 switches the model to Lion automatically, while all existing behavior stays the same when Lion is not enabled. This works for both the VAE and GAN configurations used in the homework.\n\nOverall, this addition makes it easy to compare Adam and Lion in the same codebase and experiment with how a more modern optimizer performs, all while keeping changes minimal and easy to toggle.\n\nHere are the zip files of the extended homework and solutions:",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 46,
        "unique_view_count": 17,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-08T12:21:19.006498+11:00",
        "updated_at": "2025-12-19T10:37:24.88175+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 833750,
            "role": "user",
            "name": "Nazar Ospanov",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7416258,
        "user_id": 1770931,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 515,
        "type": "post",
        "title": "Special participation D : Adding Lion to HW06",
        "content": "<document version=\"2.0\"><paragraph>I implemented Lion on HW06, after adding Manifold MuOn too to have more methods to compare. Questions associated to this new section are: </paragraph><paragraph><bold>Question 10 :</bold> Briefly explain how Lion's update is different from AdamW in terms of:</paragraph><list style=\"unordered\"><list-item><paragraph>number of state tensors per parameter</paragraph></list-item><list-item><paragraph>use (or non-use) of the gradient magnitude.</paragraph></list-item></list><paragraph><bold>Question 11</bold> : Uncomment the Lion contribution to the optimizer dictionnary. Compare Lion and AdamW:</paragraph><list style=\"unordered\"><list-item><paragraph>Which optimizer reaches lower training loss after 5 epochs?</paragraph></list-item><list-item><paragraph>Which optimizer achieves higher test accuracy?</paragraph></list-item><list-item><paragraph>Does Lion seem to converge faster early in training, slower, or about the same?</paragraph></list-item></list><paragraph/><file url=\"https://static.us.edusercontent.com/files/aaKZytCv8mbmfO8SnDsws0H7\" filename=\"Fantine_q_coding_muon_solutions_ManifoldMuOn+Lion.ipynb\"/><paragraph>The resulting plot is : </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/294ghAgzbBNHFO2nLObhJraU\" width=\"658\" height=\"325.67676767676767\"/></figure><paragraph/></document>",
        "document": "I implemented Lion on HW06, after adding Manifold MuOn too to have more methods to compare. Questions associated to this new section are: \n\nQuestion 10 : Briefly explain how Lion's update is different from AdamW in terms of:\n\nnumber of state tensors per parameter\n\nuse (or non-use) of the gradient magnitude.\n\nQuestion 11 : Uncomment the Lion contribution to the optimizer dictionnary. Compare Lion and AdamW:\n\nWhich optimizer reaches lower training loss after 5 epochs?\n\nWhich optimizer achieves higher test accuracy?\n\nDoes Lion seem to converge faster early in training, slower, or about the same?\n\n\n\nThe resulting plot is : \n\n",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 84,
        "unique_view_count": 51,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-06T09:14:09.027368+11:00",
        "updated_at": "2025-12-19T17:45:00.831474+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 1770931,
            "role": "user",
            "name": "Fantine Mpacko Priso",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7412717,
        "user_id": 582199,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 582199,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 503,
        "type": "post",
        "title": "Special Participation D: HW6 Exploration of the Polar Express (Muon Variant) & Lion Optimizers",
        "content": "<document version=\"2.0\"><paragraph>I added four additional parts to the Implementing Muon question in homework 6, providing a guided exploration of two recent computationally-efficient optimizer alternatives to Muon and AdamW:</paragraph><list style=\"bullet\"><list-item><paragraph> <link href=\"https://arxiv.org/abs/2505.16932\"><bold>Polar Express</bold></link> (Muon Variant) and</paragraph></list-item><list-item><paragraph> <link href=\"https://arxiv.org/abs/2302.06675\"><bold>Lion</bold></link> (EvoLved Sign Momentum)</paragraph></list-item></list><paragraph>New question summary:</paragraph><list style=\"number\"><list-item><paragraph>(Code) An introduction to Polar Express, a variant of Muon that replaces Newton-Schulz orthogonalization with a schedule of minimax-optimized polynomials. Since compute efficiency is a core feature of this variant, the question challenges you to find an efficient (minimal matmuls) PyTorch implementation for the update $$X_{k+1}\\leftarrow  \\alpha_k X_k+ \\beta_k X_k^3 + \\gamma_k X_k^5$$</paragraph></list-item><list-item><paragraph>(Code) An intro to the Lion optimizer, a memory-efficient, non-adaptive optimizer that relies on the sign function of an interpolation of momentum and gradient tensors to determine the update direction.</paragraph></list-item><list-item><paragraph>(Written) Empirical evaluation of Muon+PolarExpress, Muon+Newton-Schulz, Lion, AdamW, and SGD.</paragraph></list-item><list-item><paragraph>(Optional) Hyperparameter tuning for Polar Express and Lion to ensure fair comparisons.</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/KD7Wh9TXKrFaxk9WhzhtPFO3\" width=\"659\" height=\"326.17171717171715\"/></figure><paragraph>I've included the blank and solution notebooks for you below. </paragraph><paragraph>Colab links:</paragraph><list style=\"bullet\"><list-item><paragraph><link href=\"https://colab.research.google.com/github/nraultwang/cs182/blob/main/hw06/code/NRW_SP-D_q_coding_muon_BLANK.ipynb\">blank notebook</link></paragraph></list-item><list-item><paragraph><link href=\"https://colab.research.google.com/github/nraultwang/cs182/blob/main/hw06/code/NRW_SP-D_q_coding_muon_SOLUTION.ipynb\">solution notebook</link></paragraph></list-item></list><paragraph>Notebook files:</paragraph><file url=\"https://static.us.edusercontent.com/files/af5PCw5MAtKRTsAzGVaLxGwF\" filename=\"NRW_SP-D_q_coding_muon_BLANK.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/Ux7IFCbxWaBpSiJL1NhsYKWp\" filename=\"NRW_SP-D_q_coding_muon_SOLUTION.ipynb\"/><paragraph>Links for the archive:</paragraph><paragraph>Personal website: <link href=\"https://nraultwang.github.io/\">https://nraultwang.github.io/</link>, Github: <link href=\"https://github.com/nraultwang\">https://github.com/nraultwang</link></paragraph></document>",
        "document": "I added four additional parts to the Implementing Muon question in homework 6, providing a guided exploration of two recent computationally-efficient optimizer alternatives to Muon and AdamW:\n\n Polar Express (Muon Variant) and\n\n Lion (EvoLved Sign Momentum)\n\nNew question summary:\n\n(Code) An introduction to Polar Express, a variant of Muon that replaces Newton-Schulz orthogonalization with a schedule of minimax-optimized polynomials. Since compute efficiency is a core feature of this variant, the question challenges you to find an efficient (minimal matmuls) PyTorch implementation for the update $$X_{k+1}\\leftarrow  \\alpha_k X_k+ \\beta_k X_k^3 + \\gamma_k X_k^5$$\n\n(Code) An intro to the Lion optimizer, a memory-efficient, non-adaptive optimizer that relies on the sign function of an interpolation of momentum and gradient tensors to determine the update direction.\n\n(Written) Empirical evaluation of Muon+PolarExpress, Muon+Newton-Schulz, Lion, AdamW, and SGD.\n\n(Optional) Hyperparameter tuning for Polar Express and Lion to ensure fair comparisons.\n\nI've included the blank and solution notebooks for you below. \n\nColab links:\n\nblank notebook\n\nsolution notebook\n\nNotebook files:\n\nLinks for the archive:\n\nPersonal website: https://nraultwang.github.io/, Github: https://github.com/nraultwang",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 1,
        "view_count": 90,
        "unique_view_count": 46,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-05T18:22:52.659984+11:00",
        "updated_at": "2025-12-19T07:52:03.941797+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 582199,
            "role": "user",
            "name": "Nicolas Rault-Wang",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7411740,
        "user_id": 1751464,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 492,
        "type": "post",
        "title": "Special Participation D: HW7 RNN task using AdamW vs. SOAP",
        "content": "<document version=\"2.0\"><paragraph>I created a student assignment notebook focused on the SOAP (<bold>S</bold>hampo<bold>O</bold> with <bold>A</bold>dam in the <bold>P</bold>reconditioner's eigenbasis) optimizer, consisting of two main parts:</paragraph><list style=\"ordered\"><list-item><paragraph>Mathematical Implementation (Gradient Rotation):</paragraph><list style=\"unordered\"><list-item><paragraph>Designed a coding task where students implement the core SOAP operation: projecting the gradient into the eigenbasis (G<sub>projected</sub>\u200b=Q<sub>L</sub><sup>T</sup>\u200b\u22c5G\u22c5Q<sub>R</sub>\u200b).</paragraph></list-item><list-item><paragraph>Provided a <code>SimpleSOAP</code> optimizer wrapper that integrates this student-written function to simulate matrix preconditioning.</paragraph></list-item></list></list-item><list-item><paragraph>Experimental Analysis (RNN Stress Test):</paragraph><list style=\"unordered\"><list-item><paragraph>Set up the \"Adding Problem\" (a standard RNN benchmark) to test optimizer stability on long-term dependencies.</paragraph></list-item><list-item><paragraph>Constructed a Hyperparameter Sensitivity Sweep comparing AdamW vs. SOAP across logarithmically spaced learning rates.</paragraph></list-item><list-item><paragraph>Included visualization code to demonstrate SOAP's superior stability and \"shifted\" optimal learning rate window compared to AdamW.</paragraph></list-item></list></list-item></list><paragraph>Student notebook: <link href=\"https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk\">https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk</link></paragraph><paragraph>Solutions:</paragraph><paragraph><link href=\"https://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP\">https://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP</link></paragraph><paragraph/></document>",
        "document": "I created a student assignment notebook focused on the SOAP (ShampoO with Adam in the Preconditioner's eigenbasis) optimizer, consisting of two main parts:\n\nMathematical Implementation (Gradient Rotation):\n\nDesigned a coding task where students implement the core SOAP operation: projecting the gradient into the eigenbasis (Gprojected\u200b=QLT\u200b\u22c5G\u22c5QR\u200b).\n\nProvided a SimpleSOAP optimizer wrapper that integrates this student-written function to simulate matrix preconditioning.\n\nExperimental Analysis (RNN Stress Test):\n\nSet up the \"Adding Problem\" (a standard RNN benchmark) to test optimizer stability on long-term dependencies.\n\nConstructed a Hyperparameter Sensitivity Sweep comparing AdamW vs. SOAP across logarithmically spaced learning rates.\n\nIncluded visualization code to demonstrate SOAP's superior stability and \"shifted\" optimal learning rate window compared to AdamW.\n\nStudent notebook: https://colab.research.google.com/drive/15gAh01QQKscfbBbXvnGt7MgYOC3j4lTG#scrollTo=ZCwSKgXWW-nk\n\nSolutions:\n\nhttps://colab.research.google.com/drive/1-AdCaTn1n6D5ThprVc7sM0A3b7TIx09n#scrollTo=uNGvVyYhYJzP\n\n",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 85,
        "unique_view_count": 50,
        "vote_count": 0,
        "reply_count": 1,
        "unresolved_count": 1,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-05T14:59:51.904185+11:00",
        "updated_at": "2025-12-19T16:19:33.732991+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 1751464,
            "role": "user",
            "name": "Tianqu He",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7410990,
        "user_id": 1770931,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 487,
        "type": "post",
        "title": "Special participation D : addind manifold MuOn to HW06",
        "content": "<document version=\"2.0\"><paragraph>I implemented Manifold MuOn (Bernstein, 2025) at the end of HW06 to give another version of MuOn to compare. Feel free to experiment ! These are the final figures obtained after completing the code:</paragraph><file url=\"https://static.us.edusercontent.com/files/vc6ZlrQyL8Z8A4lxQ5oy0af2\" filename=\"Fantine_q_coding_muon_solutions.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/IjYgNLdp7QpZ1ulBLzN5R6qW\" filename=\"Fantine_q_coding_muon (1).ipynb\"/><figure><image src=\"https://static.us.edusercontent.com/files/kTb82SAh9dgfh34krpPNSn9v\" width=\"658\" height=\"325.67676767676767\"/></figure></document>",
        "document": "I implemented Manifold MuOn (Bernstein, 2025) at the end of HW06 to give another version of MuOn to compare. Feel free to experiment ! These are the final figures obtained after completing the code:",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 68,
        "unique_view_count": 39,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-05T13:04:37.879759+11:00",
        "updated_at": "2025-12-19T17:44:53.573495+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 1770931,
            "role": "user",
            "name": "Fantine Mpacko Priso",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7409159,
        "user_id": 961692,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 961692,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 471,
        "type": "post",
        "title": "Special Participation D: HW 4 Lion vs AdamW on CNN transfer learning and Newton-Schulz coefficients",
        "content": "<document version=\"2.0\"><paragraph>For HW4, I designed a problem to implement a reusable train_validation loop, visualise the effect of two commonly used sets of Newton-Schulz coefficients on matrix singular values, implement Lion optimizer from scratch which culminates in a systematic comparison of Lion vs AdamW on transfer learning of ResNet18 on the CIFAR-10 dataset.</paragraph><paragraph>There is a three-part structure to the problem:</paragraph><paragraph>Training Infrastructure</paragraph><list style=\"unordered\"><list-item><paragraph>Implement a reusable train_validation_loop() function for PyTorch</paragraph></list-item><list-item><paragraph>Test on SimpleCNN (not to be implemented by students) to validate correctness</paragraph></list-item></list><paragraph>Newton-Schulz Iterations</paragraph><list style=\"unordered\"><list-item><paragraph>Implement Newton-Schulz iterations to compute orthogonalized version of the input matrix</paragraph></list-item><list-item><paragraph>Visualize how aggressive vs. stable coefficients affect singular value convergence</paragraph></list-item><list-item><paragraph>The importance of orthogonalization for gradient flow and why Muon can't optimize a large number of parameters in CNNs (2D-only constraint)</paragraph></list-item><list-item><paragraph>Ending analysis questions based on Newton-Schulz coefficients and convergence</paragraph></list-item></list><paragraph>Lion Optimizer Implementation &amp; Comparison</paragraph><list style=\"unordered\"><list-item><paragraph>Implement Lion optimizer based on the original paper and test on SimpleCNN</paragraph></list-item><list-item><paragraph>Do hyperparameter grid search which tests various combinations of learning rate, batch size and weight decay (overall 27 combinations). Weight decay was added based on the suggestion at #394 </paragraph></list-item><list-item><paragraph>Train ResNet18 with best Lion config vs AdamW baseline and evaluating both models on train, validation progression and test set inference set.</paragraph><file url=\"https://static.us.edusercontent.com/files/HqoWceEYM9Wz7F8pt4RUPreJ\" filename=\"HW4_NS_CNN_Optimizer_Problem.ipynb\"/></list-item></list><file url=\"https://static.us.edusercontent.com/files/CapSY0bCXHMhNJF5q3Jt3VdD\" filename=\"HW4_NS_CNN_Optimizer_Solutions.ipynb\"/></document>",
        "document": "For HW4, I designed a problem to implement a reusable train_validation loop, visualise the effect of two commonly used sets of Newton-Schulz coefficients on matrix singular values, implement Lion optimizer from scratch which culminates in a systematic comparison of Lion vs AdamW on transfer learning of ResNet18 on the CIFAR-10 dataset.\n\nThere is a three-part structure to the problem:\n\nTraining Infrastructure\n\nImplement a reusable train_validation_loop() function for PyTorch\n\nTest on SimpleCNN (not to be implemented by students) to validate correctness\n\nNewton-Schulz Iterations\n\nImplement Newton-Schulz iterations to compute orthogonalized version of the input matrix\n\nVisualize how aggressive vs. stable coefficients affect singular value convergence\n\nThe importance of orthogonalization for gradient flow and why Muon can't optimize a large number of parameters in CNNs (2D-only constraint)\n\nEnding analysis questions based on Newton-Schulz coefficients and convergence\n\nLion Optimizer Implementation & Comparison\n\nImplement Lion optimizer based on the original paper and test on SimpleCNN\n\nDo hyperparameter grid search which tests various combinations of learning rate, batch size and weight decay (overall 27 combinations). Weight decay was added based on the suggestion at #394 \n\nTrain ResNet18 with best Lion config vs AdamW baseline and evaluating both models on train, validation progression and test set inference set.",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 96,
        "unique_view_count": 47,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-05T09:01:59.669103+11:00",
        "updated_at": "2025-12-19T10:39:55.990021+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 961692,
            "role": "user",
            "name": "Manhar Gupta",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7389679,
        "user_id": 923022,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 923022,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 395,
        "type": "post",
        "title": "Special Participation D: MuP in Hw7",
        "content": "<document version=\"2.0\"><paragraph>I created an extension to Q1 of HW7 that tries to reinforce the ideas in MuP in the context of RNNs. There are two main parts of this notebook:</paragraph><paragraph>1. Personally, I always found it confusing whether the correct MuP initialization was 1 / n, or 1 / sqrt(n), or something else, so the first part tries to empirically examine the correct initialization scheme  and connect this to an eigenvalue analysis for RNNs.</paragraph><paragraph>2. The second part empirically shows how per-layer learning rates in an RNN can help for better hyper-parameter transfer on a dataset.</paragraph><paragraph>I created both a solutions and student notebook, attached below</paragraph><file url=\"https://static.us.edusercontent.com/files/aiQ7ZfbXgMhNMmjVMJrYaauQ\" filename=\"mup_rnn_student.ipynb\"/><file url=\"https://static.us.edusercontent.com/files/AfYIRbbphIKlysXHgFcatVH5\" filename=\"mup_rnn_solution.ipynb\"/></document>",
        "document": "I created an extension to Q1 of HW7 that tries to reinforce the ideas in MuP in the context of RNNs. There are two main parts of this notebook:\n\n1. Personally, I always found it confusing whether the correct MuP initialization was 1 / n, or 1 / sqrt(n), or something else, so the first part tries to empirically examine the correct initialization scheme  and connect this to an eigenvalue analysis for RNNs.\n\n2. The second part empirically shows how per-layer learning rates in an RNN can help for better hyper-parameter transfer on a dataset.\n\nI created both a solutions and student notebook, attached below",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 138,
        "unique_view_count": 67,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-02T18:26:25.496528+11:00",
        "updated_at": "2025-12-19T10:44:46.970747+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 923022,
            "role": "user",
            "name": "Ishir Garg",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7387144,
        "user_id": 1751485,
        "course_id": 84647,
        "original_id": null,
        "editor_id": null,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 385,
        "type": "post",
        "title": "Special Participation D: Comparative Study of Muon & \u00b5P (and other optimizers) for GNN Training on Zachary\u2019s Karate Club",
        "content": "<document version=\"2.0\"><paragraph>In this write-up, I systematically benchmarked several modern optimization approaches for training a Graph Convolutional Network (GCN) on the Zachary\u2019s Karate Club graph dataset, including SGD, \u00b5P, Muon, a Muonvariant, SOAP, and Lion. I evaluated them using training/validation loss, test accuracy, convergence speed, and computational efficiency, and I also implemented an early-stopping setup and described key optimizer mechanics (e.g., Muon\u2019s Newton\u2013Schulz orthogonalization; \u00b5P-style parameter-group learning-rate scaling).<break/><break/>My main findings are that <bold>Muon can reach the best final accuracy (up to 100%)</bold>, while <bold>\u00b5P offers the best speed\u2013accuracy trade-off</bold> (fast convergence with strong accuracy), and SOAP/Lion tend to perform poorly in this small full-graph training regime.<break/><break/>The Github repository: <break/><break/><link href=\"https://github.com/hysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club\">hysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club</link></paragraph><file url=\"https://static.us.edusercontent.com/files/k8tEqNVKDfguFvOPhyKDZEGx\" filename=\"special_participation_D_HW6.pdf\"/></document>",
        "document": "In this write-up, I systematically benchmarked several modern optimization approaches for training a Graph Convolutional Network (GCN) on the Zachary\u2019s Karate Club graph dataset, including SGD, \u00b5P, Muon, a Muonvariant, SOAP, and Lion. I evaluated them using training/validation loss, test accuracy, convergence speed, and computational efficiency, and I also implemented an early-stopping setup and described key optimizer mechanics (e.g., Muon\u2019s Newton\u2013Schulz orthogonalization; \u00b5P-style parameter-group learning-rate scaling).\n\nMy main findings are that Muon can reach the best final accuracy (up to 100%), while \u00b5P offers the best speed\u2013accuracy trade-off (fast convergence with strong accuracy), and SOAP/Lion tend to perform poorly in this small full-graph training regime.\n\nThe Github repository: \n\nhysteri1a/EECS182-Comparative-Study-of-Modern-Optimizers-Muon-P-for-GNN-Training-on-Zachary-s-Karate-Club",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 77,
        "unique_view_count": 47,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-12-02T11:57:02.93077+11:00",
        "updated_at": "2025-12-19T18:15:21.143796+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 1751485,
            "role": "user",
            "name": "Tianhao Qian",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7267863,
        "user_id": 961809,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 961809,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 247,
        "type": "post",
        "title": "Special Participation D second submission HW4 Joseph Berry",
        "content": "<document version=\"2.0\"><paragraph>I edited HW4 to include a section about AdaMuon and how changing batch size and learning rate may effect performance of MLP vs CNN<break/><break/>I edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.<break/><break/><break/>Si, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" <italic>arXiv</italic>, 18 Aug. 2025, <link href=\"https://doi.org/10.48550/arXiv.2507.11005\">https://doi.org/10.48550/arXiv.2507.11005</link>.<break/><break/>Jordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" <italic>Keller Jordan Blog</italic>, 8 Dec. 2024, <link href=\"http://kellerjordan.github.io/posts/muon/\">kellerjordan.github.io/posts/muon/</link>.</paragraph><paragraph>Bernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" <italic>arXiv</italic>, 6 Dec. 2024, <link href=\"https://doi.org/10.48550/arXiv.2409.20325\">https://doi.org/10.48550/arXiv.2409.20325</link>.</paragraph><file url=\"https://static.us.edusercontent.com/files/Y2KUv4fOfq8g4IkYJ55nkO3F\" filename=\"edge_detection_sol-AdaMuon+LR_and_BatchSize_Added.ipynb\"/></document>",
        "document": "I edited HW4 to include a section about AdaMuon and how changing batch size and learning rate may effect performance of MLP vs CNN\n\nI edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.\n\n\nSi, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" arXiv, 18 Aug. 2025, https://doi.org/10.48550/arXiv.2507.11005.\n\nJordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" Keller Jordan Blog, 8 Dec. 2024, kellerjordan.github.io/posts/muon/.\n\nBernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" arXiv, 6 Dec. 2024, https://doi.org/10.48550/arXiv.2409.20325.",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 70,
        "unique_view_count": 34,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-11-07T05:40:26.301835+11:00",
        "updated_at": "2025-12-20T05:40:35.914236+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 961809,
            "role": "user",
            "name": "Joe Berry",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    },
    {
        "id": 7265176,
        "user_id": 961809,
        "course_id": 84647,
        "original_id": null,
        "editor_id": 961809,
        "accepted_id": null,
        "duplicate_id": null,
        "number": 241,
        "type": "post",
        "title": "Special Participation D HW 3 thread Joseph Berry",
        "content": "<document version=\"2.0\"><paragraph>I edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.<break/><break/>The new code and questions-solutions are at the bottom of the file<break/><break/>Si, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" <italic>arXiv</italic>, 18 Aug. 2025, <link href=\"https://doi.org/10.48550/arXiv.2507.11005\">https://doi.org/10.48550/arXiv.2507.11005</link>.<break/><break/>Jordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" <italic>Keller Jordan Blog</italic>, 8 Dec. 2024, kellerjordan.github.io/posts/muon/.</paragraph><file url=\"https://static.us.edusercontent.com/files/SezlsDhMogkxgbEaqAmtw0JF\" filename=\"q_mup_coding_sol-EDITED_LR_BATCH+Muon_Vars.ipynb\"/><paragraph>Huang, Feihu, et al. \"LiMuon: Light and Fast Muon Optimizer for Large Models.\" <italic>arXiv</italic>, 19 Sept. 2025, <link href=\"https://doi.org/10.48550/arXiv.2509.14562\">https://doi.org/10.48550/arXiv.2509.14562</link>.</paragraph><paragraph/><paragraph>Bernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" <italic>arXiv</italic>, 6 Dec. 2024, <link href=\"https://doi.org/10.48550/arXiv.2409.20325\">https://doi.org/10.48550/arXiv.2409.20325</link>.</paragraph></document>",
        "document": "I edited HW 3 Solutions to include the Muon variants AdaMuon and LiMuon and to display how changing learning rates and batch sizes effect the RMS norms.\n\nThe new code and questions-solutions are at the bottom of the file\n\nSi, Chongjie, et al. \"AdaMuon: Adaptive Muon Optimizer.\" arXiv, 18 Aug. 2025, https://doi.org/10.48550/arXiv.2507.11005.\n\nJordan, Keller, et al. \"Muon: An Optimizer for Hidden Layers in Neural Networks.\" Keller Jordan Blog, 8 Dec. 2024, kellerjordan.github.io/posts/muon/.\n\nHuang, Feihu, et al. \"LiMuon: Light and Fast Muon Optimizer for Large Models.\" arXiv, 19 Sept. 2025, https://doi.org/10.48550/arXiv.2509.14562.\n\n\n\nBernstein, Jeremy, and Laker Newhouse. \"Old Optimizer, New Norm: An Anthology.\" arXiv, 6 Dec. 2024, https://doi.org/10.48550/arXiv.2409.20325.",
        "category": "Curiosity",
        "subcategory": "",
        "subsubcategory": "",
        "flag_count": 0,
        "star_count": 0,
        "view_count": 76,
        "unique_view_count": 37,
        "vote_count": 0,
        "reply_count": 0,
        "unresolved_count": 0,
        "is_locked": false,
        "is_pinned": false,
        "is_private": false,
        "is_endorsed": false,
        "is_answered": false,
        "is_student_answered": false,
        "is_staff_answered": false,
        "is_archived": false,
        "is_anonymous": false,
        "is_megathread": false,
        "anonymous_comments": false,
        "approved_status": "approved",
        "created_at": "2025-11-06T14:41:00.93811+11:00",
        "updated_at": "2025-12-19T13:27:34.288336+11:00",
        "deleted_at": null,
        "pinned_at": null,
        "anonymous_id": 0,
        "vote": 0,
        "is_seen": false,
        "is_starred": false,
        "is_watched": null,
        "glanced_at": null,
        "new_reply_count": 0,
        "duplicate_title": null,
        "user": {
            "id": 961809,
            "role": "user",
            "name": "Joe Berry",
            "avatar": null,
            "course_role": "student",
            "tutorials": {}
        }
    }
]